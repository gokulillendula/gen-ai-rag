Production-Grade GenAI Assistant with RAG

ğŸ“Œ Overview

This project implements a production-style GenAI-powered Chat Assistant using Retrieval-Augmented Generation (RAG).

The assistant retrieves relevant document chunks using embedding-based semantic search and generates grounded responses using Gemini LLM.

The system operates in strict RAG mode to prevent hallucinations.

ğŸ— Architecture Diagram
<img width="1881" height="564" alt="Screenshot (19)" src="https://github.com/user-attachments/assets/6292626b-7b3a-4966-8b75-154abee21670" />

ğŸ”„ RAG Workflow Explanation

1ï¸âƒ£ Indexing Phase (Runs at Server Startup)
  â€¢	Load docs.json
  
  â€¢	Split documents into chunks (~400 words with overlap)
  
  â€¢	Generate embeddings using Gemini embedding model

  â€¢	Store embeddings in in-memory vector store
  
  â€¢	This happens once when the server starts.


2ï¸âƒ£ Query Phase (Runs Per Request)
When a user sends a message:
1.	Generate embedding for user query
2.	Perform cosine similarity search
3.	Retrieve top 3 most relevant chunks
4.	Apply similarity threshold (0.6)
5.	If no relevant chunks â†’ return fallback
6.	Inject retrieved context into structured prompt
7.	Generate response using Gemini LLM

Embedding Strategy

â€¢	Model used: gemini-embedding-001

â€¢	Each document is chunked before embedding

â€¢	Each chunk gets its own embedding vector

â€¢	Why chunking?

â€¢	Improves retrieval precision

â€¢	Prevents injecting large irrelevant documents

â€¢	Optimizes context window usage

ğŸ“ŠSimilarity Search Explanation

Cosine similarity formula:

similarity = (A Â· B) / (||A|| ||B||)

Why cosine similarity?

â€¢	Measures semantic closeness between vectors

â€¢	Standard in embedding-based systems

â€¢	Efficient and scalable

Top 3 chunks are selected with threshold filtering to ensure relevance.

ğŸ“ Prompt Design Reasoning

Prompt used:
You are a helpful AI assistant.
Answer ONLY using the context below.
If the answer is not in the context, say you don't know.

Context:
{retrieved_chunks}

Conversation History:
{last_3_message_pairs}

Question:
{user_query}

Design Decisions:

Strict grounding prevents hallucination

Explicit instruction to avoid external knowledge

Short conversation memory (3 pairs)

Low temperature (0.2) for deterministic answers

ğŸ”’ Hallucination Prevention

Embedding-based retrieval (no keyword search)

Similarity threshold filtering

Strict fallback response

Controlled temperature (0.2)

If similarity score is too low:

"I don't have enough information to answer that."

ğŸ”Œ API Endpoint

POST /chat/

Request
{
  "sessionId": "abc123",
  "message": "How can I reset my password?"
}

Response

{
  "reply": "Users can reset their password from Settings > Security...",
  "tokensUsed": 132,
  "retrievedChunks": 2
}

âš™ï¸ Setup Instructions

1ï¸âƒ£ Clone the Repository

git clone <your_repo_url>

cd gen-ai-assistant

2ï¸âƒ£ Create Virtual Environment

python -m venv venv

venv\Scripts\activate

3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

4ï¸âƒ£ Add Gemini API Key

Create .env file:

GEMINI_API_KEY=your_api_key_here

5ï¸âƒ£ Run Server

python manage.py runserver

Open:

http://127.0.0.1:8000/

ğŸ–¥ Frontend Features

â€¢	Chat UI

â€¢	Session handling via localStorage

â€¢	Loading indicator

â€¢	Message history display

â€¢	Scroll to bottom

ğŸš€ Features Implemented

â€¢	Real embedding-based retrieval

â€¢	Cosine similarity search

â€¢	Threshold filtering

â€¢	Strict RAG mode

â€¢	Token usage tracking

â€¢	Session-based memory

â€¢	Clean Django API


Session-based memory

Clean Django API

Interactive chat frontend
